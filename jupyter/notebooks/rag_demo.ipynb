{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea6bef7",
   "metadata": {},
   "source": [
    "# 🦜⛓️ RAG with LangChain Demo\n",
    "\n",
    "This notebook demonstrates building a robust RAG system using LangChain framework:\n",
    "- **🦜⛓️ LangChain**: Industry-standard framework for LLM applications\n",
    "- **📄 Document Processing**: Advanced text splitters and loaders\n",
    "- **🔢 Vector Embeddings**: Ollama integration with mxbai-embed-large model\n",
    "- **🗂️ Vector Storage**: Qdrant vector store integration, Qdrant Dashboard: http://localhost:6333/dashboard\n",
    "- **🤖 LLM Integration**: Ollama LLM with conversation chains\n",
    "- **🔍 Phoenix Tracing**: End-to-end observability and debugging, Phoenix Dashboard: http://localhost:6006\n",
    "- **💬 Chat Interface**: Conversational RAG with memory\n",
    "\n",
    "\n",
    "## 📖 **Sample Document**\n",
    "We'll use a comprehensive document about \"Vector stores and modern data analytics\" as our knowledge base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae140f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Phoenix setup with full LangChain and Ollama instrumentation\n",
    "phoenix_initialized = False\n",
    "try:\n",
    "    import phoenix as px\n",
    "    from phoenix.otel import register\n",
    "    \n",
    "    print(\"🔍 Setting up comprehensive Phoenix tracing...\")\n",
    "    \n",
    "    # Configure Phoenix tracing endpoint\n",
    "    phoenix_endpoint = \"http://phoenix:4317\"\n",
    "    \n",
    "    # Register Phoenix tracer for RAG project\n",
    "    tracer_provider = register(\n",
    "        project_name=\"langchain-rag-demo\",\n",
    "        endpoint=phoenix_endpoint,\n",
    "        auto_instrument=True,\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Phoenix tracer registered for LangChain RAG demo\")\n",
    "    print(f\"📡 Traces will be sent to: {phoenix_endpoint}\")\n",
    "    print(f\"🌐 Phoenix Dashboard: http://localhost:6006\")\n",
    "    phoenix_initialized = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Phoenix instrumentation not available: {e}\")\n",
    "    print(\"💡 Make sure openinference-instrumentation-langchain is installed\")\n",
    "    phoenix_initialized = False\n",
    "except Exception as e:\n",
    "    print(f\"❌ Phoenix setup failed: {e}\")\n",
    "    phoenix_initialized = False\n",
    "\n",
    "print(f\"\\n🔧 RAG System Status:\")\n",
    "print(f\"LangChain Framework: ✅ Enabled\")\n",
    "print(f\"Vector Database: ✅ Enabled\") \n",
    "print(f\"LLM & Embeddings: ✅ Enabled\")\n",
    "print(f\"Observability: {'✅ Enabled' if phoenix_initialized else '❌ Disabled'}\")\n",
    "\n",
    "if phoenix_initialized:\n",
    "    print(f\"\\n📊 Phoenix Tracing Features:\")\n",
    "    print(f\"  🔗 LangChain chains and agents\")\n",
    "    print(f\"  🤖 Ollama LLM calls and embeddings\")\n",
    "    print(f\"  📄 Document retrieval operations\")\n",
    "    print(f\"  💭 Conversation memory tracking\")\n",
    "    print(f\"  🧮 Token usage and latency metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries and LangChain Components\n",
    "print(\"📦 Importing required libraries...\")\n",
    "\n",
    "# Standard library imports\n",
    "import time\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# OpenTelemetry for Phoenix tracing\n",
    "from opentelemetry import trace\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# LangChain Community Components  \n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "# LangChain Integration Components\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "print(\"✅ All required libraries imported successfully!\")\n",
    "print(\"🦜⛓️ LangChain components ready\")\n",
    "print(\"🔧 Text processing utilities loaded\")\n",
    "print(\"🤖 Ollama integrations available\")\n",
    "print(\"🗂️ Vector store connectors ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc396668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service health and available models\n",
    "import requests\n",
    "print(\"🔍 RAG Service Health Check:\\n\")\n",
    "\n",
    "services = {\n",
    "    \"Ollama LLM & Embeddings\": \"http://ollama:11434/api/tags\",\n",
    "    \"Qdrant Vector Database\": \"http://qdrant:6333/\",\n",
    "    \"Phoenix AI Observability\": \"http://phoenix:6006/health\"\n",
    "}\n",
    "\n",
    "service_status = {}\n",
    "for service, url in services.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✅ {service}: Healthy\")\n",
    "            service_status[service] = True\n",
    "        else:\n",
    "            print(f\"⚠️ {service}: Responding but status {response.status_code}\")\n",
    "            service_status[service] = False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ {service}: Not accessible ({e})\")\n",
    "        service_status[service] = False\n",
    "\n",
    "print(\"\\n🤖 Available Ollama Models:\")\n",
    "try:\n",
    "    response = requests.get(\"http://ollama:11434/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        available_models = []\n",
    "        for model in models:\n",
    "            model_name = model.get('name', 'Unknown')\n",
    "            print(f\"  📋 {model_name}\")\n",
    "            available_models.append(model_name)\n",
    "        \n",
    "        # Check for required models\n",
    "        required_models = ['gemma3:4b', 'mxbai-embed-large:latest']\n",
    "        models_ready = all(any(req in model for model in available_models) for req in required_models)\n",
    "        print(f\"\\n🎯 Required models available: {'✅ Yes' if models_ready else '❌ No'}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"  ⚠️ Could not retrieve model list\")\n",
    "        models_ready = False\n",
    "except:\n",
    "    print(\"  ❌ Ollama not accessible\")\n",
    "    models_ready = False\n",
    "\n",
    "# Store global status for later use\n",
    "SERVICES_READY = service_status.get(\"Ollama LLM & Embeddings\", False) and \\\n",
    "                service_status.get(\"Qdrant Vector Database\", False) and \\\n",
    "                models_ready\n",
    "\n",
    "print(f\"\\n🚀 System Ready for RAG: {'✅ Yes' if SERVICES_READY else '❌ No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f8801",
   "metadata": {},
   "source": [
    "## 2. Sample Document - Modern Data Analytics Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced sample document about modern data analytics\n",
    "SAMPLE_DOCUMENT = \"\"\"\n",
    "# Modern Data Analytics: A Comprehensive Guide\n",
    "\n",
    "## Introduction to Data Analytics\n",
    "\n",
    "Data analytics is the science of analyzing raw data to make conclusions about information. It involves applying algorithmic or mechanical processes to derive insights and encompasses a variety of techniques and methodologies used across industries to improve decision-making processes.\n",
    "\n",
    "The field has evolved significantly with the advent of big data, cloud computing, and artificial intelligence, transforming how organizations extract value from their data assets.\n",
    "\n",
    "## Key Components of Modern Data Analytics\n",
    "\n",
    "### 1. Data Collection and Storage\n",
    "\n",
    "Modern data analytics begins with robust data collection systems. Organizations typically use data lakes and data warehouses to store structured and unstructured data. Cloud-based solutions like Amazon S3, Google Cloud Storage, and Azure Data Lake have become popular choices for scalable storage solutions.\n",
    "\n",
    "Data lakes allow for storing raw data in its native format until needed, while data warehouses provide structured, processed data optimized for analytical queries. The modern approach often combines both in a lakehouse architecture.\n",
    "\n",
    "### 2. Data Processing and ETL\n",
    "\n",
    "Extract, Transform, Load (ETL) processes are crucial for preparing data for analysis. Modern tools like Apache Spark, Apache Kafka, and cloud-based services enable real-time and batch processing of large datasets. These tools can handle petabytes of data across distributed systems.\n",
    "\n",
    "ETL has evolved into ELT (Extract, Load, Transform) in many cloud environments, where raw data is loaded first and then transformed as needed, providing more flexibility and faster ingestion.\n",
    "\n",
    "### 3. Machine Learning and AI Integration\n",
    "\n",
    "Machine learning has become integral to modern data analytics. Techniques such as supervised learning, unsupervised learning, and reinforcement learning help identify patterns and make predictions. Popular frameworks include TensorFlow, PyTorch, and scikit-learn.\n",
    "\n",
    "AutoML platforms are democratizing machine learning by automating model selection, hyperparameter tuning, and feature engineering, making advanced analytics accessible to non-experts.\n",
    "\n",
    "### 4. Data Visualization and Business Intelligence\n",
    "\n",
    "Effective data visualization transforms complex data into understandable insights. Tools like Tableau, Power BI, Apache Superset, and custom dashboards help stakeholders make data-driven decisions. Interactive visualizations enable exploration of data from multiple perspectives.\n",
    "\n",
    "Modern BI tools incorporate AI-powered features like natural language queries, automated insights, and predictive analytics directly in the visualization layer.\n",
    "\n",
    "### 5. Real-time Analytics and Stream Processing\n",
    "\n",
    "Modern businesses require real-time insights to respond quickly to changing conditions. Stream processing frameworks like Apache Kafka, Apache Flink, and Apache Storm enable organizations to process and analyze data as it arrives.\n",
    "\n",
    "Real-time analytics powers applications like fraud detection, recommendation engines, IoT monitoring, and dynamic pricing systems.\n",
    "\n",
    "## Vector Databases and Similarity Search\n",
    "\n",
    "Vector databases have emerged as a crucial component for AI-powered applications. They store high-dimensional vectors representing embeddings of text, images, or other data types. These databases enable similarity search, recommendation systems, and retrieval-augmented generation (RAG) applications.\n",
    "\n",
    "Popular vector databases include:\n",
    "- **Qdrant**: High-performance vector database with excellent API design and hybrid search capabilities\n",
    "- **Pinecone**: Managed vector database service with easy scaling\n",
    "- **Weaviate**: Open-source vector database with GraphQL interface and semantic search\n",
    "- **Chroma**: Lightweight vector database perfect for AI applications and prototyping\n",
    "- **Milvus**: Open-source vector database designed for scalable similarity search\n",
    "\n",
    "Vector databases solve the challenge of finding semantically similar content, enabling applications like document search, image recognition, and recommendation systems.\n",
    "\n",
    "## Applications of Modern Data Analytics\n",
    "\n",
    "### Healthcare Analytics\n",
    "\n",
    "In healthcare, data analytics helps improve patient outcomes through predictive modeling, drug discovery, and personalized treatment plans. Electronic health records (EHR) analysis can identify treatment patterns and potential health risks.\n",
    "\n",
    "Applications include predicting patient readmissions, optimizing treatment protocols, drug interaction analysis, and population health management.\n",
    "\n",
    "### Financial Services\n",
    "\n",
    "Financial institutions use analytics for fraud detection, risk assessment, algorithmic trading, and customer behavior analysis. Real-time transaction monitoring helps prevent fraudulent activities.\n",
    "\n",
    "Advanced applications include credit scoring, regulatory compliance monitoring, market risk analysis, and robo-advisory services for investment management.\n",
    "\n",
    "### E-commerce and Retail\n",
    "\n",
    "Retail analytics enables personalized recommendations, inventory optimization, price optimization, and customer segmentation. Understanding customer behavior helps improve conversion rates and customer satisfaction.\n",
    "\n",
    "Modern retail analytics incorporates omnichannel data, real-time personalization, demand forecasting, and supply chain optimization.\n",
    "\n",
    "### Manufacturing and IoT\n",
    "\n",
    "In manufacturing, analytics help optimize production processes, predict equipment failures, and improve quality control. Internet of Things (IoT) sensors generate vast amounts of data that can be analyzed for operational insights.\n",
    "\n",
    "Industry 4.0 initiatives leverage analytics for predictive maintenance, digital twins, supply chain visibility, and autonomous quality control systems.\n",
    "\n",
    "## Best Practices for Data Analytics\n",
    "\n",
    "### 1. Data Quality Management\n",
    "\n",
    "Ensure data accuracy, completeness, and consistency through proper validation and cleaning processes. Poor data quality leads to unreliable insights and poor decision-making.\n",
    "\n",
    "Implement data quality frameworks with automated monitoring, profiling, and cleansing processes throughout the data pipeline.\n",
    "\n",
    "### 2. Security and Privacy\n",
    "\n",
    "Implement robust security measures to protect sensitive data. Comply with regulations like GDPR, CCPA, and industry-specific requirements. Use encryption, access controls, and audit trails.\n",
    "\n",
    "Privacy-preserving techniques like differential privacy, federated learning, and synthetic data generation are becoming increasingly important.\n",
    "\n",
    "### 3. Scalability and Performance\n",
    "\n",
    "Design systems that can scale with growing data volumes and user demands. Use distributed computing frameworks and cloud-native architectures for flexibility.\n",
    "\n",
    "Consider serverless architectures, containerization, and microservices for elastic scaling and cost optimization.\n",
    "\n",
    "### 4. Collaboration and Documentation\n",
    "\n",
    "Foster collaboration between data scientists, analysts, and business stakeholders. Maintain proper documentation of data pipelines, models, and analytical processes.\n",
    "\n",
    "Implement DataOps practices to improve collaboration, automate workflows, and ensure reproducible analytics.\n",
    "\n",
    "## Future Trends in Data Analytics\n",
    "\n",
    "The field of data analytics continues to evolve rapidly. Emerging trends include:\n",
    "\n",
    "- **Automated machine learning (AutoML)** making AI more accessible to business users\n",
    "- **Edge analytics** bringing computation closer to data sources for real-time processing\n",
    "- **Explainable AI** providing transparency in model decisions and building trust\n",
    "- **Quantum computing** potentially revolutionizing complex calculations and optimization\n",
    "- **Real-time decision-making systems** powered by streaming analytics and AI\n",
    "- **Augmented analytics** using AI to assist analysts in data preparation and insight discovery\n",
    "- **Data mesh architectures** decentralizing data ownership and management\n",
    "- **Synthetic data generation** for privacy-preserving analytics and model training\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Modern data analytics combines traditional statistical methods with cutting-edge technologies to extract value from data. Success requires the right combination of tools, processes, and skilled professionals. Organizations that effectively leverage data analytics gain competitive advantages through better decision-making, operational efficiency, and customer insights.\n",
    "\n",
    "The future of data analytics lies in making advanced capabilities more accessible, automated, and integrated into business processes, while maintaining strong governance and ethical standards.\n",
    "\"\"\"\n",
    "\n",
    "print(\"📄 Enhanced Sample Document Loaded:\")\n",
    "print(f\"📊 Document Length: {len(SAMPLE_DOCUMENT):,} characters\")\n",
    "print(f\"📊 Word Count: ~{len(SAMPLE_DOCUMENT.split()):,} words\")\n",
    "print(f\"📊 Lines: {len(SAMPLE_DOCUMENT.splitlines())}\")\n",
    "\n",
    "print(\"\\n📖 Document Structure:\")\n",
    "lines = SAMPLE_DOCUMENT.split('\\n')\n",
    "headers = [line for line in lines if line.startswith('#')]\n",
    "for header in headers[:10]:  # Show first 10 headers\n",
    "    level = header.count('#')\n",
    "    print(f\"{'  ' * (level-1)}📌 {header.strip('#').strip()}\")\n",
    "\n",
    "print(f\"\\n📖 Document Preview:\")\n",
    "print(\"=\"*80)\n",
    "print(SAMPLE_DOCUMENT[:500] + \"...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77570a4a",
   "metadata": {},
   "source": [
    "## 4. Document Processing with LangChain Text Splitters\n",
    "\n",
    "LangChain provides sophisticated text splitters that preserve semantic meaning and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain Document objects and split them\n",
    "print(\"🔧 Processing document with LangChain text splitters...\")\n",
    "\n",
    "# Create a Document object\n",
    "doc = Document(\n",
    "    page_content=SAMPLE_DOCUMENT,\n",
    "    metadata={\n",
    "        \"source\": \"modern_data_analytics_guide.md\",\n",
    "        \"title\": \"Modern Data Analytics: A Comprehensive Guide\",\n",
    "        \"author\": \"DataLab Playground\",\n",
    "        \"created_at\": \"2025-01-31\",\n",
    "        \"document_type\": \"knowledge_base\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter for intelligent chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Target chunk size\n",
    "    chunk_overlap=200,  # Overlap between chunks to preserve context\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n\\n\",  # Paragraph breaks (highest priority)\n",
    "        \"\\n\",    # Line breaks\n",
    "        \" \",     # Word breaks\n",
    "        \"\"       # Character breaks (fallback)\n",
    "    ],\n",
    "    add_start_index=True  # Add start index to metadata\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "documents = text_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"\\n📊 Document Processing Results:\")\n",
    "print(f\"Original Document Length: {len(SAMPLE_DOCUMENT):,} characters\")\n",
    "print(f\"Total Chunks Created: {len(documents)}\")\n",
    "\n",
    "# Analyze chunk statistics\n",
    "chunk_lengths = [len(doc.page_content) for doc in documents]\n",
    "print(f\"Average Chunk Size: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "print(f\"Min Chunk Size: {min(chunk_lengths)} characters\")\n",
    "print(f\"Max Chunk Size: {max(chunk_lengths)} characters\")\n",
    "\n",
    "# Display sample chunks with metadata\n",
    "print(f\"\\n📖 Sample Chunks with Metadata:\")\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Length: {len(doc.page_content)} characters\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content Preview: {doc.page_content[:200]}...\")\n",
    "    \n",
    "    # Show overlap with previous chunk\n",
    "    if i > 0:\n",
    "        prev_content = documents[i-1].page_content\n",
    "        current_content = doc.page_content\n",
    "        \n",
    "        # Find overlapping text\n",
    "        overlap_found = False\n",
    "        for j in range(min(len(prev_content), 300), 0, -1):\n",
    "            if prev_content[-j:] in current_content[:j+50]:\n",
    "                print(f\"Overlap detected: '{prev_content[-j:]}...'\")\n",
    "                overlap_found = True\n",
    "                break\n",
    "        \n",
    "        if not overlap_found:\n",
    "            print(\"No overlap detected\")\n",
    "\n",
    "print(f\"\\n✅ Document processing complete! Ready for vectorization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575769e",
   "metadata": {},
   "source": [
    "## 5. Vector Store Setup with LangChain-Qdrant Integration\n",
    "\n",
    "We'll use LangChain's Qdrant integration for seamless vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama Embeddings with Phoenix Tracing\n",
    "print(\"🔢 Setting up Ollama Embeddings...\")\n",
    "\n",
    "# Get tracer for embedding operations\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "with tracer.start_as_current_span(\"initialize_embeddings\") as span:\n",
    "    span.set_attribute(\"embedding.model\", \"mxbai-embed-large\")\n",
    "    span.set_attribute(\"embedding.base_url\", \"http://ollama:11434\")\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(\n",
    "        base_url=\"http://ollama:11434\",\n",
    "        model=\"mxbai-embed-large:latest\",\n",
    "    )\n",
    "    \n",
    "    # Test embeddings\n",
    "    try:\n",
    "        print(\"🧪 Testing embeddings...\")\n",
    "        with tracer.start_as_current_span(\"test_embeddings\") as test_span:\n",
    "            test_text = \"This is a test for embeddings\"\n",
    "            test_span.set_attribute(\"test.text\", test_text)\n",
    "            \n",
    "            test_embedding = embeddings.embed_query(test_text)\n",
    "            \n",
    "            test_span.set_attribute(\"embedding.dimension\", len(test_embedding))\n",
    "            test_span.set_attribute(\"embedding.success\", True)\n",
    "            \n",
    "            print(f\"✅ Embeddings working! Dimension: {len(test_embedding)}\")\n",
    "            print(f\"📊 Sample values: {test_embedding[:5]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        span.set_attribute(\"embedding.error\", str(e))\n",
    "        span.set_attribute(\"embedding.success\", False)\n",
    "        print(f\"❌ Embeddings test failed: {e}\")\n",
    "        print(\"💡 Make sure Ollama service is running with mxbai-embed-large model\")\n",
    "\n",
    "print(\"🗂️ Setting up Qdrant vector store with LangChain...\")\n",
    "\n",
    "try:\n",
    "    \n",
    "    collection_name = \"langchain_data_analytics\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"connect_vector_store\") as span:\n",
    "        span.set_attribute(\"vector_store.type\", \"qdrant\")\n",
    "        span.set_attribute(\"vector_store.collection\", collection_name)\n",
    "        span.set_attribute(\"vector_store.url\", \"http://qdrant:6333\")\n",
    "        \n",
    "        try:\n",
    "            # LangChain's QdrantVectorStore automatically handles:\n",
    "            # - Qdrant client connection\n",
    "            # - Collection creation with correct vector dimensions\n",
    "            # - Distance metric configuration (defaults to cosine)\n",
    "            vector_store = QdrantVectorStore.from_existing_collection(\n",
    "                embedding=embeddings,\n",
    "                collection_name=collection_name,\n",
    "                url=\"http://qdrant:6333\"\n",
    "            )\n",
    "            \n",
    "            span.set_attribute(\"vector_store.connection\", \"existing\")\n",
    "            span.set_attribute(\"vector_store.success\", True)\n",
    "            print(f\"✅ Connected to existing collection '{collection_name}'\")\n",
    "            \n",
    "        except Exception:\n",
    "            # If collection doesn't exist, create it from documents\n",
    "            span.set_attribute(\"vector_store.connection\", \"new\")\n",
    "            print(f\"🆕 Creating new collection '{collection_name}' from documents...\")\n",
    "            \n",
    "            try:\n",
    "                # This will automatically:\n",
    "                # - Create Qdrant client\n",
    "                # - Determine vector size from embeddings\n",
    "                # - Set up collection with optimal settings\n",
    "                # - Index the provided documents\n",
    "                vector_store = QdrantVectorStore.from_documents(\n",
    "                    documents=[],  # Empty for now, we'll add documents later\n",
    "                    embedding=embeddings,\n",
    "                    url=\"http://qdrant:6333\",\n",
    "                    collection_name=collection_name,\n",
    "                    prefer_grpc=False\n",
    "                )\n",
    "                span.set_attribute(\"vector_store.success\", True)\n",
    "                print(f\"✅ Collection '{collection_name}' created successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                span.set_attribute(\"vector_store.error\", str(e))\n",
    "                span.set_attribute(\"vector_store.success\", False)\n",
    "                print(f\"❌ Error creating vector store: {e}\")\n",
    "                print(\"💡 Make sure Qdrant service is running and accessible\")\n",
    "                vector_store = None\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up vector store: {e}\")\n",
    "    vector_store = None\n",
    "\n",
    "if vector_store:\n",
    "    # Get collection stats using LangChain's built-in methods\n",
    "    try:\n",
    "        # Simple way to check collection status\n",
    "        print(f\"\\n📋 Vector Store Ready:\")\n",
    "        print(f\"  Collection: {collection_name}\")\n",
    "        print(f\"  Embedding Model: mxbai-embed-large (1024 dims)\")\n",
    "        print(f\"  Distance Metric: Cosine\")\n",
    "        print(f\"  Status: ✅ Connected\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not get collection details: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae86e4",
   "metadata": {},
   "source": [
    "## 6. Document Indexing and Vector Storage\n",
    "\n",
    "Add all document chunks to the vector store with LangChain's batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to vector store\n",
    "\n",
    "if vector_store:\n",
    "    print(\"📚 Indexing documents in vector store...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if documents are already in the vector store\n",
    "        # Use similarity search to test if there are existing documents\n",
    "        try:\n",
    "            test_results = vector_store.similarity_search(\"test\", k=1)\n",
    "            current_count = len(test_results) if test_results else 0\n",
    "            has_documents = current_count > 0\n",
    "        except Exception:\n",
    "            # If similarity search fails, assume empty collection\n",
    "            has_documents = False\n",
    "            current_count = 0\n",
    "        \n",
    "        if has_documents:\n",
    "            print(f\"📊 Found existing documents in collection\")\n",
    "            reindex = input(\"🔄 Re-index documents? (y/N): \").lower().strip() == 'y'\n",
    "            if not reindex:\n",
    "                print(\"⏭️ Skipping indexing - using existing documents\")\n",
    "            else:\n",
    "                print(\"🗑️ Will overwrite with new documents\")\n",
    "        else:\n",
    "            reindex = True\n",
    "        \n",
    "        if not has_documents or reindex:\n",
    "            print(f\"🚀 Indexing {len(documents)} document chunks...\")\n",
    "            \n",
    "            # Add documents to vector store in batches\n",
    "            batch_size = 5  # Process in small batches to avoid overwhelming the service\n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch = documents[i:i + batch_size]\n",
    "                \n",
    "                print(f\"📥 Processing batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}\")\n",
    "                \n",
    "                # Add batch to vector store\n",
    "                vector_store.add_documents(batch)\n",
    "                \n",
    "                # Small delay between batches\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Verify indexing with a test search\n",
    "            test_results = vector_store.similarity_search(\"data analytics\", k=3)\n",
    "            print(f\"\\n✅ Indexing complete!\")\n",
    "            print(f\"📊 Test search returned {len(test_results)} results\")\n",
    "            \n",
    "            # Show sample of what was indexed\n",
    "            if test_results:\n",
    "                print(f\"\\n📖 Sample indexed documents:\")\n",
    "                for i, doc in enumerate(test_results[:2]):\n",
    "                    print(f\"\\n--- Document {i+1} ---\")\n",
    "                    print(f\"Content: {doc.page_content[:150]}...\")\n",
    "                    print(f\"Metadata: {doc.metadata}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during indexing: {e}\")\n",
    "        vector_store = None\n",
    "else:\n",
    "    print(\"❌ Vector store not available - skipping indexing\")\n",
    "\n",
    "print(f\"\\n🎯 Vector store ready: {'✅ Yes' if vector_store else '❌ No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d325f0",
   "metadata": {},
   "source": [
    "## 7. LangChain RAG Components - Retriever and Chains\n",
    "\n",
    "Now we'll build the core RAG components using LangChain's modular architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083832d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain retriever from vector store\n",
    "if vector_store is not None:\n",
    "    print(\"🔗 Creating LangChain RAG components...\")\n",
    "    \n",
    "    # Create retriever with search configuration\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": 4,  # Number of documents to retrieve\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Retriever created successfully\")\n",
    "    \n",
    "    # Test retriever\n",
    "    test_query = \"What are vector databases used for?\"\n",
    "    try:\n",
    "        retrieved_docs = retriever.invoke(test_query)  # Updated method name\n",
    "        \n",
    "        print(f\"\\n🧪 Retriever Test:\")\n",
    "        print(f\"Query: '{test_query}'\")\n",
    "        print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_docs[:2]):\n",
    "            print(f\"\\n--- Document {i+1} ---\")\n",
    "            print(f\"Content: {doc.page_content[:200]}...\")\n",
    "            print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Retriever test failed: {e}\")\n",
    "        print(\"💡 This might be normal if no documents are indexed yet\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Vector store not available - cannot create retriever\")\n",
    "    retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom prompt template for RAG\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    base_url=\"http://ollama:11434\",\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Define a comprehensive prompt template\n",
    "qa_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer based on the context, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me help you based on the information provided. \"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=qa_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Custom prompt template created\")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "if retriever is not None and 'llm' in locals():\n",
    "    print(\"🔗 Creating RetrievalQA chain...\")\n",
    "    \n",
    "    try:\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",  # Stuff all retrieved docs into the prompt\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": qa_prompt}\n",
    "        )\n",
    "        \n",
    "        print(\"✅ RetrievalQA chain created successfully\")\n",
    "        \n",
    "        # Test the QA chain\n",
    "        test_question = \"What are vector databases used for and which one is the best in your opinion?\"\n",
    "        \n",
    "        print(f\"\\n🧪 Testing RetrievalQA chain...\")\n",
    "        print(f\"Question: '{test_question}'\")\n",
    "        \n",
    "        try:\n",
    "            result = qa_chain.invoke({\"query\": test_question})  # Updated method\n",
    "            \n",
    "            print(f\"\\n🤖 Answer:\")\n",
    "            print(result[\"result\"])\n",
    "            \n",
    "            print(f\"\\n📚 Source Documents ({len(result['source_documents'])}):\")\n",
    "            for i, doc in enumerate(result[\"source_documents\"]):\n",
    "                print(f\"\\n--- Source {i+1} ---\")\n",
    "                print(f\"Content: {doc.page_content[:200]}...\")\n",
    "                print(f\"Metadata: {doc.metadata}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ QA chain test failed: {e}\")\n",
    "            print(\"💡 Check if LLM is properly initialized and accessible\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create QA chain: {e}\")\n",
    "        qa_chain = None\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Cannot create QA chain - missing retriever or LLM\")\n",
    "    print(\"💡 Make sure both retriever and llm variables are defined\")\n",
    "    qa_chain = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fb873",
   "metadata": {},
   "source": [
    "## 8. Conversational RAG with Memory\n",
    "\n",
    "Add conversation memory for multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e69731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversational RAG chain with memory\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    base_url=\"http://ollama:11434\",\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0.1,\n",
    "    num_predict=8096,  # Maximum output tokens (increase for longer responses)\n",
    "    num_ctx=4096,      # Context window size\n",
    "    top_p=0.9,         # Nucleus sampling\n",
    "    top_k=40,          # Top-k sampling\n",
    "    repeat_penalty=1.1 # Reduce repetition\n",
    ")\n",
    "\n",
    "print(\"✅ Ollama LLM initialized\")\n",
    "\n",
    "if retriever is not None and 'llm' in locals():\n",
    "    print(\"💭 Creating Conversational RAG chain with memory...\")\n",
    "    \n",
    "    try:\n",
    "        # Create conversation memory\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "        \n",
    "        # Create conversational retrieval chain\n",
    "        conversational_qa = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            memory=memory,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Conversational RAG chain created successfully\")\n",
    "        \n",
    "        # Test conversational flow\n",
    "        print(f\"\\n🧪 Testing Conversational RAG...\")\n",
    "        \n",
    "        try:\n",
    "            # First question\n",
    "            q1 = \"What are vector databases?\"\n",
    "            print(f\"\\n👤 Question 1: {q1}\")\n",
    "            \n",
    "            result1 = conversational_qa.invoke({\"question\": q1})\n",
    "            print(f\"🤖 Answer 1: {result1['answer']}\")\n",
    "            \n",
    "            # Follow-up question\n",
    "            q2 = \"How are they used in AI applications?\"\n",
    "            print(f\"\\n👤 Question 2: {q2}\")\n",
    "            \n",
    "            result2 = conversational_qa.invoke({\"question\": q2})\n",
    "            print(f\"🤖 Answer 2: {result2['answer']}\")\n",
    "            \n",
    "            # Check memory\n",
    "            print(f\"\\n💭 Conversation Memory:\")\n",
    "            print(f\"Messages in memory: {len(memory.chat_memory.messages)}\")\n",
    "            \n",
    "            # Show source documents from last question\n",
    "            if 'source_documents' in result2:\n",
    "                print(f\"\\n📚 Sources for last answer ({len(result2['source_documents'])}):\")\n",
    "                for i, doc in enumerate(result2['source_documents'][:2]):\n",
    "                    print(f\"  📄 Source {i+1}: {doc.page_content}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Conversational RAG test failed: {e}\")\n",
    "            print(\"💡 Check if LLM is responding properly\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create conversational chain: {e}\")\n",
    "        conversational_qa = None\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Cannot create conversational chain - missing components\")\n",
    "    print(\"💡 Make sure both retriever and llm variables are defined\")\n",
    "    conversational_qa = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c5d62",
   "metadata": {},
   "source": [
    "## 9. Advanced RAG Demo Functions\n",
    "\n",
    "Interactive functions for testing the complete RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb43021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str, use_conversational: bool = False):\n",
    "    \"\"\"\n",
    "    Ask a question using either simple QA or conversational RAG.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"❓ Question: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if use_conversational and conversational_qa is not None:\n",
    "        print(\"🗣️ Using Conversational RAG...\")\n",
    "        result = conversational_qa({\"question\": question})\n",
    "        answer = result[\"answer\"]\n",
    "        sources = result.get(\"source_documents\", [])\n",
    "    elif qa_chain is not None:\n",
    "        print(\"🔍 Using Simple RAG...\")\n",
    "        result = qa_chain({\"query\": question})\n",
    "        answer = result[\"result\"]\n",
    "        sources = result.get(\"source_documents\", [])\n",
    "    else:\n",
    "        print(\"❌ RAG system not available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🤖 Answer:\")\n",
    "    print(f\"{answer}\")\n",
    "    \n",
    "    if sources:\n",
    "        print(f\"\\n📚 Sources ({len(sources)} documents):\")\n",
    "        for i, doc in enumerate(sources):\n",
    "            print(f\"\\n--- Source {i+1} ---\")\n",
    "            print(f\"Content: {doc.page_content[:250]}...\")\n",
    "            if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "def test_rag_capabilities():\n",
    "    \"\"\"\n",
    "    Test the RAG system with various types of questions.\n",
    "    \"\"\"\n",
    "    print(\"🎯 Testing RAG System Capabilities\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_questions = [\n",
    "        {\n",
    "            \"question\": \"What is data analytics?\",\n",
    "            \"category\": \"Definition\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the key components of modern data analytics?\",\n",
    "            \"category\": \"Components\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do vector databases work with embeddings?\",\n",
    "            \"category\": \"Technical\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are some applications in healthcare?\",\n",
    "            \"category\": \"Application\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the best practices for data quality?\",\n",
    "            \"category\": \"Best Practices\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test in enumerate(test_questions):\n",
    "        print(f\"\\n🧪 Test {i+1}: {test['category']}\")\n",
    "        ask_question(test[\"question\"])\n",
    "        \n",
    "        if i < len(test_questions) - 1:\n",
    "            print(f\"\\n{'⏳ Moving to next test...'}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat session with the RAG system.\n",
    "    \"\"\"\n",
    "    print(\"💬 Interactive RAG Chat Session\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Ask questions about data analytics!\")\n",
    "    print(\"Commands: 'quit' to exit, 'memory' to see conversation history\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\n👤 Your question: \").strip()\n",
    "            \n",
    "            if question.lower() == 'quit':\n",
    "                print(\"👋 Goodbye! Thanks for using the RAG system.\")\n",
    "                break\n",
    "            \n",
    "            if question.lower() == 'memory':\n",
    "                if conversational_qa and hasattr(conversational_qa, 'memory'):\n",
    "                    print(f\"\\n💭 Conversation History:\")\n",
    "                    messages = conversational_qa.memory.chat_memory.messages\n",
    "                    for i, msg in enumerate(messages[-6:]):  # Show last 6 messages\n",
    "                        print(f\"{i+1}. {type(msg).__name__}: {str(msg)[:100]}...\")\n",
    "                else:\n",
    "                    print(\"💭 No conversation memory available\")\n",
    "                continue\n",
    "            \n",
    "            if not question:\n",
    "                print(\"❓ Please enter a question\")\n",
    "                continue\n",
    "            \n",
    "            ask_question(question, use_conversational=True)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n👋 Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Example usage\n",
    "print(\"🎉 LangChain RAG System Ready!\")\n",
    "print(\"\\n🚀 Available functions:\")\n",
    "print(\"1. ask_question('Your question here')\")\n",
    "print(\"2. test_rag_capabilities()\")\n",
    "print(\"3. interactive_chat()\")\n",
    "\n",
    "# Run a quick demonstration\n",
    "print(\"\\n🧪 Quick Demo:\")\n",
    "if qa_chain:\n",
    "    ask_question(\"What are the future trends in data analytics?\")\n",
    "else:\n",
    "    print(\"❌ RAG system not properly initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8199fc-b81e-4f68-ad12-0f9faf27dc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ff274-2325-4ab8-bc9c-46add2eefd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI DEV",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
